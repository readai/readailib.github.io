<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Ml on ReadAILib Official Website </title>
    <link>/categories/ml/</link>
    <description>Recent content in Ml on ReadAILib Official Website </description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 21 Dec 2017 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/categories/ml/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>单变量线性回归</title>
      <link>/2017/12/21/single-variable-linear-regression/</link>
      <pubDate>Thu, 21 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/12/21/single-variable-linear-regression/</guid>
      <description>该笔记是来自 Andrew Ng 的 Machine Learning 课程的第二周:单变量线性回归的课堂记录，主要讲解了以下几个内容:
代价函数梯度下降算法模型表达还是上一个关于房屋价格预测的例子，给你一组房屋的大小和对应房屋的价格的数据，让你对数据进行建模，使得对于其他的房屋的大小更好的拟合出更准确的价格出来。如图1所示。图 1: 房屋价格预测模型题外话：首先数据是一组关于标准输入与标准答案的数据集，那么对该数据进行建模，是属于监督学习任务。又因为数据只含有一个变量（\(size\)）即一个特征，且找到一条直线来拟合数据集，所以该问题又被称为单变量线性回归问题。
图2是部分的样本数据，其中\(X\)表示房屋的大小，\(Y\)表示对应房屋的价格。图 2: 数据为了后面更好的描述，所以对一些符号进行声明：
\(m\) 训练集中实例的数量\(x&amp;#39;s\) 输入变量或者特征\(y&amp;#39;s\) 输出变量或目标变量\((x,y)\) 一个训练样本\((x^{(i)},y^{(i)})\) 第\(i\)个训练样本 \end{itemize}有了数据之后，可以用下图就可以描述房屋价格预测问题建模的过程。图 3: 建模的过程图片描述：因而解决房屋价格预测问题，实际上就是要将训练集”喂给“我们的学习算法，进而学习到一个假设\(h\)，然后将要预测的房屋尺寸作为输入变量输入给\(h\)，预测出该房屋的交易价格作为输出结果。其实\(h\)就是一个我们要学习的函数。
我们如何表示函数呢？对于单变量线性回归问题来说，\(h_{\theta}(x) = \theta_{0} + \theta_{1} x\).
代价函数引入代价函数有了训练数据，也有了模型的表示\(h\)，但是问题是如何选择 \(\theta_{0}\) 和 \(\theta_{1}\) ?
基本想法是选择\(\theta_{0}\)，\(\theta_{1}\)以使得在训练集上\(h_{\theta}(x)\)接近\(y\).用数学表达式表示出来，如下： \[minimize_{\theta{0},\theta_{1}}\frac{1}{2m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})- y^{(i)})^{2}\]
注意： 在学习的时候，有一个区别，\(loss function\)描述的是单个样本误差，而\(cost\) \(function\)描述整个训练集的误差。
同样地，为了更好的下文描述，将其简化：
\[J(\theta_{0},\theta_{1}) = \frac{1}{2m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})- y^{(i)})^{2}\]</description>
    </item>
    
    <item>
      <title>多变量线性回归</title>
      <link>/2017/12/21/multi-variable-linear-regression/</link>
      <pubDate>Thu, 21 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/12/21/multi-variable-linear-regression/</guid>
      <description>该笔记是来自 Andrew Ng 的 Machine Learning 课程的第二周:多变量线性回归的课堂记录，其实就是将单变量线性回归推广到多变量中去，主要讲解了以下几个内容:
模型的表示梯度下降算法运算中的实用技巧特征缩放学习率\(\alpha\)模型表达在单变量线性回归的房屋预测模型例子中，我们忽略了一个显示，往往购房者不仅仅考虑一个因素，因此多变量（特征）的引入是必需的，比如房屋的价格往往和卧室的数量（numbers of bedrooms），有多少个楼层(numbers of floors)，房屋的年龄(age of home)等等，我们选取这些特征来预测价格，实例数据如图为了后面描述的方便，特别对符号作出声明：
\(n\) 特征的数目\(m\) 训练集中实例的数量\(x&amp;#39;s\) 输入变量或者特征\(y&amp;#39;s\) 输出变量或目标变量\(x^{(i)}\) 第\(i\)个训练样本的输入特征\(x^{(i)}_{j}\) 第\(i\)个训练样本的第\(j\)个特征\(x_{j}\) 样本数据的第\(j\)个特征如图中，第一个训练样本为\(x^{(1)}=\left[\begin{matrix} 2104 &amp;amp;5 &amp;amp;1&amp;amp; 45 &amp;amp;460\end{matrix}\right]\) 则第一个训练样本的第一个特征为\(x^{(1)}_{1} = 21040\)样本数据的第一个特征 \(x_{1}= \left[ \begin{matrix} 2104 \\ 1416 \\ 1534\\ 852 \end{matrix} \right]\)
同样地，支持多变量的假说函数表示为\[h_{\theta}(x) = \theta_{0} + \theta_{1}x_{1} + \theta_{2}x_{2} + \theta_{3}x_{3} + .</description>
    </item>
    
    <item>
      <title>多项式回归</title>
      <link>/2017/12/21/polynomial-regression/</link>
      <pubDate>Thu, 21 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/12/21/polynomial-regression/</guid>
      <description>该笔记是来自 Andrew Ng 的 Machine Learning 课程的第二周:多项式回归的课堂记录，有了合适的特征之后，我们发现线性回归并不适用于所有数据，有时我们需要曲线来适应我们的数据，因此引出多项式回归模型，主要讲解了以下几个内容:
将多项式回归模型转换为线性模型正规方程将多项式回归模型转换为线性模型什么是多项式模型？比如一个二次模型\[h_{\theta} = \theta_{0} + \theta_{1}x + \theta_{2}x^2\],三次模型\[h_{\theta} = \theta_{0} + \theta_{1}x + \theta_{2}x^2 + \theta_{3}x^3\]
通常情况下，在模型进行选择时，首先要观察数据，然后再决定准备尝试什么样的模型，比如下面图 中的数据分布，我们可以通过观察可以看到，数据的分布用线性模型并不能达到很好的效果，所以在这里尝试用一个多项式模型来拟合我们的训练数据。
可以尝试多项式函数表示我们的模型：
\[h_{\theta} = \theta_{0} + \theta_{1}x + \theta_{2}x^2\]
或者
\[h_{\theta} = \theta_{0} + \theta_{1}x + \theta_{2}x^2 + \theta_{3}x^3\]
或者
\[h_{\theta} = \theta_{0} + \theta_{1}x + \theta_{2}x^{\frac{1}{2}}\]
有趣的是，如果我们对这些高次项进行替换，比如在\(h_{\theta} = \theta_{0} + \theta_{1}x + \theta_{2}x^2\)中，将 \[x_{2} := x^{2}\] 将可以转换为\(h_{\theta} = \theta_{0} + \theta_{1}x + \theta_{2}x_{2}\)，这样转换为多变量线性回归模型了。</description>
    </item>
    
    <item>
      <title>朴素贝叶斯算法</title>
      <link>/2017/12/21/bayesian-learning/</link>
      <pubDate>Thu, 21 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/12/21/bayesian-learning/</guid>
      <description>本节主要讲解三个基本算法的之一：朴素贝叶斯算法，该算法主要是依据贝叶斯规则/公式/推理，它的主要作用在于，它和其他将要讲述的\(k\)近邻算法，感知机算法都一样，你的算法设计出来之后，至少你的算法的performance要比这三类算法的效果要好！另外贝叶斯学习算法基于一些统计上的假设而成立的，因此贝叶斯学习算法又称为统计学习的算法必须学习的算法.本节主要有以下部分内容：
Bayes’ RuleApplying Baye’s Rule to ClassificationThe Posterior Probability :\(\mathrm{P}(C|x) = \frac{\mathrm{P}(C)\mathrm{P}(x|C)}{\mathrm{P}(x)}\)Extend to Multi-class classificationNaive Bayes for ClassificationZero-frequency Problem贝叶斯定理/公式/规则假设\(\{B_1,B_2,...,B_k\}\)是\(S\)集合的划分且对于\(i=0,1,...,k\)有\(\mathrm{P}(B_i) &amp;gt; 0\)，那么 \[\mathrm{P}(B_j|A) = \frac{\mathrm{P}(A|B_j)\mathrm{P}(B_j)}{\sum_{i=1}^k \mathrm{P}(A|B_i)\mathrm{P}(B_i)}\]
在分类问题上应用贝叶斯分类器例子：信用卡评估：高风险/低风险信用卡评估是这样的一个过程，银行通过对用户的历史交易信息来进行选择的，如果该用户在过去的交易中按时缴纳了本金和利息，并且银行从中获益，那么这些用户是属于低风险的，如果他们没有按时缴纳本金和利息或者拒绝还款的话，这类用户比银行定义为高风险的用户.
我们现在的任务就是来学习如何对用户进行分类 vs 低风险.那么我们现在有以下用户的每年的收入信息和消费信息，我们们用两个随机变量 \(X_1\) 和 \(X_2\) 来表示.用户的信用度可以用伯努利随机变量\(\mathrm{C} = 1\) 和 \(\mathrm{C} = 0\) 来表示。其中\(C=0\)表示这是一个低风险的用户，\(C=0\)表示这是一个高风险的用户。
当一个新的用户来进行贷款时，他提供他的每年的收入和消费，即\(X_1 = x_1\) 和 \(X_2 = x_2\).如果我们知道信用度C的概率受限于观测值\(X = [x_1, x_2]\)，并且有 \[\begin{align*}&amp;amp;\mathrm{if} \; \; \mathrm{P}(C = 1|[x_1 , x_2 ]) &amp;gt; 0.</description>
    </item>
    
    <item>
      <title>机器学习简介</title>
      <link>/2017/12/21/introduction-machine-learning/</link>
      <pubDate>Thu, 21 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/12/21/introduction-machine-learning/</guid>
      <description>该笔记是来自 Andrew Ng 的 Machine Learning 课程的第一周的课堂记录，主要讲解了以下几个内容:
机器学习的定义机器学习的分类第一类：监督学习第二类：学习理论第三类：非监督学习第四类：强化学习什么是机器学习？非正式定义1959年， Arthur Samuel1非正式地定义了机器学习：“在不直接针对问题进行编程的情况下，赋予计算机学习能力的一个研究领域”。
西洋棋就是一个例子，西洋棋自己和自己下棋。由于计算机程序的处理速度非常快，所以Arthur Samuel让计算机与计算机自己下了成千上万盘棋，逐渐地，西洋棋意识到了怎样的局势能使自己胜利，什么样的局势导致自己失败，它会反复地自我学习：“如果让对手占据这些地方时，那么我输的概率可能比较大”，或者“如果我占据这些地方时，我胜利的概率比较大”。在1959年，奇迹出现，他的西洋棋程序的棋艺远远超过西洋棋程序的作者！
过去人们的看法是计算机除了做程序明确让其做的事情，除外它什么都不能做，但是Arthur Samuel做到了！
现代化的定义Tom Mitchell在1998年提出现代化的机器学习的定义：“一个合理的学习问题应该是这样定义的：对于一个计算机程序来说，给它一个任务\(T\)和一个性能评测方法\(P\)，如果在经验\(E\)的影响下，\(P\)对\(T\)的测量结果得到了改进，那么就说明程序从中学习到了经验\(E\).
比如对于西洋棋那个例子来说：
\(E\) - 程序成千上万次的自我练习\(T\) - 下棋\(P\) - 它与人类棋手对奕的概率监督学习回归问题下面是给予一组房屋大小与对应的房屋价格的数据进行拟合的例子：
通过学习来预测房屋价格的问题是监督学习问题的一个例子，之所以成为监督学习，是因为我们这个算法提供了一组房屋的大小(\(size\))和一组某种程度上可以堪称正确答案的房屋价格\(Price\)的数据。比如(1000, 30)等。
监督问题的学习，给算法提供了一组”正确“的输入和”标准“答案，之后，我们希望算法能够去学习标准输入和标准答案之间的联系，以尝试对于我们提供的其他输入来给我们提供更为标准的答案。
分类问题分类问题是监督学习问题中的另一类问题，它与回归问题最大的区别在于，此时的数据是离散的我而不是连续的.
下面是给予肿瘤的大小和对应的肿瘤是否是恶性（1）或者良性（0），来进行拟合学习最佳的分类决策线。其中
数据是一组关于乳腺癌的数据，\(X\)代表肿瘤的大小，\(Y\)代表肿瘤的是否为恶性目标是让一个算法学会预测一个肿瘤是否是恶性或良性很显然，这是一个2分类问题。具体如图2所示：下面考虑一组多个输入变量（多个特征）的数据，下面以两个特征进行举例说明。其中\(X\)表示肿瘤的大小，\(Y\)表示患者的年龄，对于图中的样本数据的表示进行说明：’o’表示良性，’x’表示为恶性，目标是找出最佳的分类决策边界。如图3所示：如果你的数据不能在二维，三维甚至任何有限维空间表示出来，如果你觉得数据实际上存在于无线维空间中该怎么办？
solution:支持向量机算法2，可以将数据映射到无限维空间，所以他不仅能处理像之前例子中的两个特征所表示的数据，而且还可以处理无限种特征.
学习理论学习理论主要主要试图了解一下什么算法能够很好地近似不同的函数，并且试图了解一些诸如需要多少训练集数据，测试集数据这样的问题，还比如算法优化，欠拟合和过拟合等问题。</description>
    </item>
    
    <item>
      <title>过拟合和正则化</title>
      <link>/2017/12/21/overfitting/</link>
      <pubDate>Thu, 21 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/12/21/overfitting/</guid>
      <description>该笔记是来自 Andrew Ng 的 Machine Learning 课程的第三周:多分类和过拟合技术的课堂记录，过拟合是机器学习优化的部分，主要讲解了以下几个内容:
多类别的分类问题过/欠拟合问题解决过拟合的方法正则化方法多类别的分类问题在现实生活中多类别的问题更常见，比如：
Email:邮件的自动分类，e.g. work,friends,family,hobby医疗诊断：Notil,Cold,Flu天气预测：Sunny,Cloudy,Rain,Snow在涉及到多分类问题，我们往往采取一种“One VS All”算法！在二分类问题上\(Y = \{0,1\}\)，在多分类上，我们将扩大\(Y\)的定义，即\(Y = \{0,1,…,n\}\)。多分类的任务转化为\(n + 1\)个（+ 1因为索引从0开始）二分类问题；在每一个二分类任务，预测的“\(Y\)”是一个类概率。
\[\begin{align*}&amp;amp; y \in \lbrace0, 1 ... n\rbrace \\&amp;amp; h_\theta^{(0)}(x) = P(y = 0 | x ; \theta) \\&amp;amp; h_\theta^{(1)}(x) = P(y = 1 | x ; \theta) \\&amp;amp; \cdots \\&amp;amp; h_\theta^{(n)}(x) = P(y = n | x ; \theta) \\&amp;amp; \mathrm{prediction} = \max_i( h_\theta ^{(i)}(x) )\end{align*}\]我们基本上是选择其中一个类别的数据作为positive，然后将所有其他的类别数据为negative，这样将问题转化为一个二分类问题，反复这样做，对每一种情况应用二元logistic回归，然后将所有情况中预测的最高的那个类作为我们的预测。下图 显示了如何将3个类进行分类：</description>
    </item>
    
    <item>
      <title>逻辑回归</title>
      <link>/2017/12/21/logistic-regression/</link>
      <pubDate>Thu, 21 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/12/21/logistic-regression/</guid>
      <description>该笔记是来自 Andrew Ng 的 Machine Learning 课程的第三周:逻辑回归的课堂记录，逻辑回归是机器学习分类算法的地一个算法，涵盖信息论的相关内容.主要讲解了以下几个内容:分类算法的应用分类的场景在现实社会中无处不在，比如
Email:垃圾邮件分类金融交易是否存在欺诈肿瘤是恶性/良性分类问题和回归问题一样，只是现在要预测的值只占一小部分离散值。现在，将重点讨论二分类问题，其中\(Y\)只能接受两个值，0和1。（也可以推广到多个类），例如，如果我们试图为电子邮件构建一个垃圾邮件分类器，那么\(X^{(i)}\)可能是一封电子邮件的一些特征，如果它是一封垃圾邮件，\(y\)是1，否则为0。因此，\(Y \in \{0,1\}\)。0也被称为否定类，1是正类，它们有时也用符号“-”和“+”表示。
二分类问题医疗诊断是机器学习在医学方面其中的一个应用，本小节主要借助乳腺癌诊断这个例子引出.给出一组数据，其中数据特征为肿瘤的大小，标记\(y\)是0或者1， 其中0表示恶性，1表示良性，将这些样本数据做图如下：
我们可以尝试使用线性回归加上一个合适的来实现分类问题，如图所示，\(h_{\theta}(x) = \theta^{T}x\)，则基于阈值0.5的分类器算法如下： \[\begin{align*}&amp;amp; h_\theta(x) \geq 0.5 \rightarrow y = 1 \\&amp;amp; h_\theta(x) &amp;lt; 0.5 \rightarrow y = 0 \end{align*}\]
但是这将存在一个很大问题，该算法并不具有鲁棒性，即假设我们有观测到了一个非常大的尺寸的恶性肿瘤，将其作为实例加入到我们的训练集中，这样将会我们之前的假说\(h\)发生很大变化，如图中最右侧的那个点加入到训练集中，将对\(h(x)\)影响很大，这时将0.5作为阈值来预测肿瘤是否为恶性就不再合适了。虽然很容易构造出模型来，但用原来的线性回归模型解决分类问题的这种方法表现很差。直观地说，也没有任何道理，通过图很容易看出，存在\(h_{\theta}(x)&amp;gt;1\)或者\(h_{\theta}(x)&amp;lt;0\)，但，\(Y \in \{0,1\}\)。为了解决这个问题，我们改变\(h_{\theta}(x)\)的形式，使得输出变量\(0 \leq h_{\theta}(x) \leq 1\).我们将使用\(Sigmoid\)来表示假说函数，也叫为逻辑函数，又因为逻辑函数的输入为\(\theta^{T}x\)，所以我们的模型又叫做逻辑回归模型.
逻辑回归模型为： \[\begin{align*}&amp;amp; h_\theta (x) = g ( \theta^T x )\\&amp;amp; z = \theta^T x \\&amp;amp;g(z) = \dfrac{1}{1 + e^{-z}}\end{align*}\]</description>
    </item>
    
  </channel>
</rss>